"""
CallCops: ONNX Export Script for Web/Mobile Inference
=======================================================

ONNX Runtime Web (Wasm/WebGPU) ë° ëª¨ë°”ì¼ í™˜ê²½ì„ ìœ„í•œ ìµœì í™”ëœ ë³€í™˜.

Features:
1. Encoder/Decoder ë¶„ë¦¬ export
2. Dynamic Axes (Batch, Time)
3. Opset 16 (ONNX Runtime Web í˜¸í™˜)
4. INT8 Quantization (Static + Dynamic)
5. PyTorch vs ONNX ê²€ì¦

Target:
- ONNX Runtime Web (Wasm/WebGPU)
- ONNX Runtime Mobile (iOS/Android)
- Model Size < 10MB
"""

import os
import sys
import argparse
from pathlib import Path
from typing import Optional, Tuple, Dict
import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F

# Project path
sys.path.insert(0, str(Path(__file__).parent.parent))

from models import CallCopsNet, Encoder, Decoder


# =============================================================================
# Wrapper Classes for Clean ONNX Export
# =============================================================================

class EncoderONNXWrapper(nn.Module):
    """
    Encoder ONNX Wrapper
    ====================
    
    Audio + Message â†’ Watermarked Audio
    
    ONNX í˜¸í™˜ì„ ìœ„í•´ forwardë§Œ ë…¸ì¶œ.
    """
    
    def __init__(self, encoder: Encoder):
        super().__init__()
        self.encoder = encoder
    
    def forward(
        self,
        audio: torch.Tensor,   # [B, 1, T]
        message: torch.Tensor  # [B, 128]
    ) -> torch.Tensor:
        """
        Args:
            audio: [B, 1, T] - 8kHz audio (variable length)
            message: [B, 128] - Watermark bits (0/1 float)
            
        Returns:
            watermarked: [B, 1, T] - Watermarked audio
        """
        return self.encoder(audio, message)


class DecoderONNXWrapper(nn.Module):
    """
    Decoder ONNX Wrapper v2.0
    =========================
    
    Frame-Wise Decoder compatible wrapper.
    Returns frame-wise bit probabilities.
    
    Watermarked Audio â†’ Frame-wise Bit Probabilities
    """
    
    def __init__(self, decoder: Decoder, target_length: int = 8000):
        super().__init__()
        self.decoder = decoder
        self.target_length = target_length
        
        # Frame configuration
        self.frame_samples = 320  # 40ms @ 8kHz
        self.expected_frames = target_length // self.frame_samples  # 25 frames for 1s
    
    def forward(
        self,
        audio: torch.Tensor  # [B, 1, T]
    ) -> torch.Tensor:
        """
        Args:
            audio: [B, 1, T] - Watermarked audio (fixed length = target_length)
            
        Returns:
            bit_probs: [B, num_frames] - Extracted bit probabilities per frame
        """
        # Run through decoder (new frame-wise architecture)
        logits = self.decoder(audio)  # [B, num_frames]
        
        # Sigmoid for probabilities
        probs = torch.sigmoid(logits)
        return probs


# =============================================================================
# ONNX Export Functions
# =============================================================================

def export_encoder_onnx(
    encoder: Encoder,
    output_path: Path,
    opset_version: int = 16,
    example_length: int = 8000  # 1ì´ˆ @ 8kHz
) -> Path:
    """Encoderë¥¼ ONNXë¡œ export"""
    
    wrapper = EncoderONNXWrapper(encoder)
    wrapper.eval()
    
    # Example inputs
    batch_size = 1
    example_audio = torch.randn(batch_size, 1, example_length)
    example_message = torch.randint(0, 2, (batch_size, 128)).float()
    
    # Dynamic axes for variable batch and audio length
    dynamic_axes = {
        'audio': {0: 'batch_size', 2: 'audio_length'},
        'message': {0: 'batch_size'},
        'watermarked': {0: 'batch_size', 2: 'audio_length'}
    }
    
    # Export
    torch.onnx.export(
        wrapper,
        (example_audio, example_message),
        str(output_path),
        input_names=['audio', 'message'],
        output_names=['watermarked'],
        dynamic_axes=dynamic_axes,
        opset_version=opset_version,
        do_constant_folding=True,
        export_params=True,
        verbose=False
    )
    
    print(f"âœ… Encoder exported: {output_path}")
    print(f"   Size: {output_path.stat().st_size / 1024 / 1024:.2f} MB")
    
    return output_path


def export_decoder_onnx(
    decoder: Decoder,
    output_path: Path,
    opset_version: int = 16,
    example_length: int = 8000
) -> Path:
    """Decoderë¥¼ ONNXë¡œ export (Frame-Wise v2.0)
    
    New architecture uses fixed stride Conv1d instead of AdaptiveAvgPool1d,
    so dynamic audio length is now supported!
    
    Output: [B, num_frames] where num_frames = audio_length // 320
    """
    
    wrapper = DecoderONNXWrapper(decoder, target_length=example_length)
    wrapper.eval()
    
    # Example input
    batch_size = 1
    example_audio = torch.randn(batch_size, 1, example_length)
    
    # Dynamic axes - both batch_size AND audio_length are now dynamic!
    dynamic_axes = {
        'audio': {0: 'batch_size', 2: 'audio_length'},
        'bit_probs': {0: 'batch_size', 1: 'num_frames'}
    }
    
    # Export
    torch.onnx.export(
        wrapper,
        (example_audio,),
        str(output_path),
        input_names=['audio'],
        output_names=['bit_probs'],
        dynamic_axes=dynamic_axes,
        opset_version=opset_version,
        do_constant_folding=True,
        export_params=True,
        verbose=False
    )
    
    print(f"âœ… Decoder exported: {output_path}")
    print(f"   Size: {output_path.stat().st_size / 1024 / 1024:.2f} MB")
    print(f"   Output: [B, num_frames] (num_frames = audio_length // 320)")
    
    return output_path


# =============================================================================
# INT8 Quantization
# =============================================================================

def quantize_onnx_dynamic(
    input_path: Path,
    output_path: Path
) -> Path:
    """
    ONNX Dynamic Quantization (INT8)
    
    ê°€ì¥ ê°„ë‹¨í•œ ì–‘ìí™”. ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„° ë¶ˆí•„ìš”.
    """
    try:
        from onnxruntime.quantization import quantize_dynamic, QuantType
    except ImportError:
        print("âš ï¸  onnxruntime-extensions required for quantization")
        print("   Install: pip install onnxruntime onnxruntime-extensions")
        return input_path
    
    try:
        quantize_dynamic(
            model_input=str(input_path),
            model_output=str(output_path),
            weight_type=QuantType.QUInt8,
            per_channel=False,
            reduce_range=False
        )
        print(f"âœ… Quantized (Dynamic INT8): {output_path}")
        print(f"   Size: {output_path.stat().st_size / 1024 / 1024:.2f} MB")
        
    except Exception as e:
        print(f"âš ï¸  Quantization failed: {e}")
        print("   Skipping quantization for this model.")
        if output_path.exists():
            output_path.unlink()
        return input_path
    
    return output_path


def quantize_onnx_static(
    input_path: Path,
    output_path: Path,
    calibration_data: list,
    is_encoder: bool = True
) -> Path:
    """
    ONNX Static Quantization (INT8)
    
    ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë” ì •ë°€í•œ ì–‘ìí™”.
    """
    try:
        from onnxruntime.quantization import quantize_static, CalibrationDataReader, QuantType
    except ImportError:
        print("âš ï¸  onnxruntime required for static quantization")
        return quantize_onnx_dynamic(input_path, output_path)
    
    class CallCopsCalibrationReader(CalibrationDataReader):
        def __init__(self, data_list: list, is_encoder: bool):
            self.data_list = data_list
            self.is_encoder = is_encoder
            self.index = 0
        
        def get_next(self) -> Optional[Dict]:
            if self.index >= len(self.data_list):
                return None
            
            audio = self.data_list[self.index]
            self.index += 1
            
            if self.is_encoder:
                message = np.random.randint(0, 2, (1, 128)).astype(np.float32)
                return {'audio': audio, 'message': message}
            else:
                return {'audio': audio}
    
    calibration_reader = CallCopsCalibrationReader(calibration_data, is_encoder)
    
    quantize_static(
        model_input=str(input_path),
        model_output=str(output_path),
        calibration_data_reader=calibration_reader,
        weight_type=QuantType.QInt8,
        activation_type=QuantType.QUInt8,
        per_channel=False
    )
    
    print(f"âœ… Quantized (Static INT8): {output_path}")
    print(f"   Size: {output_path.stat().st_size / 1024 / 1024:.2f} MB")
    
    return output_path


# =============================================================================
# Validation
# =============================================================================

def validate_onnx_model(
    pytorch_model: nn.Module,
    onnx_path: Path,
    is_encoder: bool = True,
    num_tests: int = 5,
    tolerance: float = 1e-4
) -> bool:
    """
    PyTorch vs ONNX ì¶œë ¥ ë¹„êµ ê²€ì¦
    """
    try:
        import onnxruntime as ort
    except ImportError:
        print("âš ï¸  onnxruntime required for validation")
        print("   Install: pip install onnxruntime")
        return False
    
    # ONNX Runtime session
    session = ort.InferenceSession(
        str(onnx_path),
        providers=['CPUExecutionProvider']
    )
    
    pytorch_model.eval()
    
    print(f"\nğŸ” Validating {onnx_path.name} against PyTorch...")
    
    all_passed = True
    max_diff = 0.0
    
    FRAME_SAMPLES = 320  # 40ms @ 8kHz
    
    for i in range(num_tests):
        # Random test input (variable length)
        if is_encoder:
            audio_length = np.random.randint(2000, 16000)  # 0.25s ~ 2s
        else:
            # Decoder: align to frame boundary for consistent comparison
            num_frames = np.random.randint(10, 50)  # 10~50 frames
            audio_length = num_frames * FRAME_SAMPLES
        
        audio_np = np.random.randn(1, 1, audio_length).astype(np.float32)
        audio_torch = torch.from_numpy(audio_np)
        
        if is_encoder:
            message_np = np.random.randint(0, 2, (1, 128)).astype(np.float32)
            message_torch = torch.from_numpy(message_np)
            
            # PyTorch inference
            with torch.no_grad():
                pytorch_out = pytorch_model(audio_torch, message_torch).numpy()
            
            # ONNX inference
            onnx_out = session.run(
                None,
                {'audio': audio_np, 'message': message_np}
            )[0]
        else:
            # PyTorch inference
            with torch.no_grad():
                logits = pytorch_model(audio_torch)
                pytorch_out = torch.sigmoid(logits).numpy()
            
            # ONNX inference
            onnx_out = session.run(None, {'audio': audio_np})[0]
        
        # Compare (handle potential shape mismatch due to tracing)
        min_len = min(pytorch_out.shape[-1], onnx_out.shape[-1])
        diff = np.abs(pytorch_out[..., :min_len] - onnx_out[..., :min_len]).max()
        max_diff = max(max_diff, diff)
        
        shape_match = "âœ“" if pytorch_out.shape == onnx_out.shape else f"âš  shapes: PT{pytorch_out.shape} vs ONNX{onnx_out.shape}"
        
        if diff > tolerance:
            print(f"   âŒ Test {i+1}: FAILED (max diff: {diff:.6f}) {shape_match}")
            all_passed = False
        else:
            print(f"   âœ… Test {i+1}: PASSED (max diff: {diff:.6f}, frames: {min_len}) {shape_match}")
    
    if all_passed:
        print(f"   âœ… All {num_tests} tests PASSED (max diff: {max_diff:.6f})")
    else:
        print(f"   âŒ Some tests FAILED")
    
    return all_passed


def check_onnx_model(onnx_path: Path) -> bool:
    """ONNX ëª¨ë¸ ìœ íš¨ì„± ê²€ì‚¬"""
    try:
        import onnx
        model = onnx.load(str(onnx_path))
        onnx.checker.check_model(model)
        print(f"   âœ… ONNX model check passed: {onnx_path.name}")
        return True
    except Exception as e:
        print(f"   âŒ ONNX model check failed: {e}")
        return False


# =============================================================================
# Main Export Pipeline
# =============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="CallCops ONNX Export for Web/Mobile",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument(
        '--checkpoint', type=str, required=True,
        help='Path to model checkpoint (.pth)'
    )
    parser.add_argument(
        '--output_dir', type=str, default='exported/onnx',
        help='Output directory for ONNX models'
    )
    parser.add_argument(
        '--opset', type=int, default=16,
        help='ONNX opset version (16+ recommended for ONNX Runtime Web)'
    )
    parser.add_argument(
        '--quantize', action='store_true',
        help='Apply INT8 quantization'
    )
    parser.add_argument(
        '--validate', action='store_true',
        help='Validate ONNX output against PyTorch'
    )
    parser.add_argument(
        '--skip_encoder', action='store_true',
        help='Skip encoder export'
    )
    parser.add_argument(
        '--skip_decoder', action='store_true',
        help='Skip decoder export'
    )
    
    args = parser.parse_args()
    
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("=" * 60)
    print("CallCops ONNX Export")
    print("=" * 60)
    
    # ========================================
    # 1. Load Checkpoint
    # ========================================
    print(f"\nğŸ“¦ Loading checkpoint: {args.checkpoint}")
    
    checkpoint = torch.load(args.checkpoint, map_location='cpu')
    config = checkpoint.get('config', {})
    
    model = CallCopsNet(
        message_dim=config.get('watermark', {}).get('payload_length', 128),
        hidden_channels=config.get('model', {}).get('hidden_channels', [32, 64, 128, 256]),
        num_residual_blocks=config.get('model', {}).get('num_residual_blocks', 4),
        use_discriminator=False  # Exportì—ëŠ” Discriminator ë¶ˆí•„ìš”
    )
    
    # Load weights (strict=False for partial load)
    model.load_state_dict(checkpoint['model_state_dict'], strict=False)
    model.eval()
    
    # Model info
    encoder_params = sum(p.numel() for p in model.encoder.parameters())
    decoder_params = sum(p.numel() for p in model.decoder.parameters())
    print(f"   Encoder parameters: {encoder_params:,}")
    print(f"   Decoder parameters: {decoder_params:,}")
    
    # ========================================
    # 2. Export Encoder
    # ========================================
    if not args.skip_encoder:
        print(f"\nğŸ”§ Exporting Encoder (Opset {args.opset})...")
        
        encoder_path = output_dir / "encoder.onnx"
        export_encoder_onnx(
            model.encoder,
            encoder_path,
            opset_version=args.opset
        )
        
        check_onnx_model(encoder_path)
        
        if args.validate:
            encoder_wrapper = EncoderONNXWrapper(model.encoder)
            validate_onnx_model(encoder_wrapper, encoder_path, is_encoder=True)
        
        if args.quantize:
            print("\nğŸ”§ Quantizing Encoder...")
            encoder_quant_path = output_dir / "encoder_int8.onnx"
            quantize_onnx_dynamic(encoder_path, encoder_quant_path)
    
    # ========================================
    # 3. Export Decoder
    # ========================================
    if not args.skip_decoder:
        print(f"\nğŸ”§ Exporting Decoder (Opset {args.opset})...")
        
        decoder_path = output_dir / "decoder.onnx"
        export_decoder_onnx(
            model.decoder,
            decoder_path,
            opset_version=args.opset
        )
        
        check_onnx_model(decoder_path)
        
        if args.validate:
            validate_onnx_model(model.decoder, decoder_path, is_encoder=False)
        
        if args.quantize:
            print("\nğŸ”§ Quantizing Decoder...")
            decoder_quant_path = output_dir / "decoder_int8.onnx"
            quantize_onnx_dynamic(decoder_path, decoder_quant_path)
    
    # ========================================
    # 4. Summary
    # ========================================
    print("\n" + "=" * 60)
    print("ğŸ“Š Export Summary")
    print("=" * 60)
    
    total_size = 0
    for f in sorted(output_dir.glob("*.onnx")):
        size_mb = f.stat().st_size / 1024 / 1024
        total_size += size_mb
        status = "âœ…" if size_mb < 10 else "âš ï¸"
        print(f"   {status} {f.name}: {size_mb:.2f} MB")
    
    print(f"\n   Total: {total_size:.2f} MB")
    
    if total_size < 20:
        print("   âœ… Size target met (< 10MB per model)")
    else:
        print("   âš ï¸  Consider quantization to reduce size")
    
    # Usage guide
    print("\n" + "=" * 60)
    print("ğŸŒ ONNX Runtime Web Usage")
    print("=" * 60)
    print("""
// JavaScript Example
import * as ort from 'onnxruntime-web';

// Load encoder
const encoder = await ort.InferenceSession.create('./encoder.onnx');

// Prepare input (8kHz audio, Float32Array)
const audioData = new Float32Array(8000);  // 1 second
const messageData = new Float32Array(128); // 128-bit watermark

// Create tensors
const audioTensor = new ort.Tensor('float32', audioData, [1, 1, 8000]);
const messageTensor = new ort.Tensor('float32', messageData, [1, 128]);

// Run inference
const result = await encoder.run({
    audio: audioTensor,
    message: messageTensor
});

const watermarkedAudio = result.watermarked.data;
""")
    
    print("\nâœ… Export completed!")


if __name__ == "__main__":
    main()
